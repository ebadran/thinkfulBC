{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U4 L5 P1 - Unsupervised Learning Capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Emile Badran - 24/July/2018**\n",
    "\n",
    "In this project, we'll conduct Latent Semantic Analysis of a data set consisting of 1.6 million tweets. The goal is to predict a hashtag based on the contents of a tweet.\n",
    "\n",
    "We'll start by creating a \"bag of words\" and \"TF-IDF\" feature sets and running supervised machine learning models to predict a tweet's hashtag. We'll then use unsupervised clustering techniques to see whether they can consistently group tweets with the same hashtags into the same cluster.\n",
    "\n",
    "Finally, we'll calculate the cosine similarity of tweets and see if the most similar tweets have the same hashtag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "We'll use a Kaggle data set consisting of 1.6 million tweets, labeled as [0 = negative, 2 = neutral, 4 = positive].\n",
    "\n",
    "More information about the data set can be found [here](http://help.sentiment140.com/for-students/). The original research paper can be found [here](https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf).\n",
    "\n",
    "Citation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12.\n",
    "\n",
    "Link to download the data set:\n",
    "https://www.kaggle.com/kazanova/sentiment140/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the SpaCy module\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  text\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
       "1      is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
       "2                            @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data set file into a data frame\n",
    "twDF = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin1',\n",
    "                       usecols=[5], names=['text'])\n",
    "\n",
    "print(twDF.shape)\n",
    "twDF.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the top hashtags\n",
    "\n",
    "Not all tweets have hashtags. We'll start by finding the top five hashtags, cleaning the data set, and creating our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert tweets to lowercase\n",
    "twDF.text = twDF.text.str.lower()\n",
    "\n",
    "# Convert tweets into lists of words, spaces and punctuation marks. We'll need this to find the top hashtags\n",
    "twDF['splitted'] = twDF.text.apply(lambda x: x.split())\n",
    "\n",
    "# Create a list of words from all tweets\n",
    "wordlist = []\n",
    "twDF.splitted.apply(lambda x: [wordlist.append(i) for i in x])\n",
    "\n",
    "# Convert lists to more efficient numpy arrays\n",
    "word_array = np.asarray(wordlist)\n",
    "\n",
    "# Delete those lists to free up memory\n",
    "del wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#followfriday', 2288),\n",
       " ('#fb', 1765),\n",
       " ('#squarespace', 867),\n",
       " ('#ff', 822),\n",
       " ('#seb-day', 498),\n",
       " ('#iranelection', 485),\n",
       " ('#', 472),\n",
       " ('#musicmonday', 397),\n",
       " ('#1', 391),\n",
       " ('#fail', 343),\n",
       " ('#asot400', 320)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most common hashtags\n",
    "top_hashtags = Counter([word for word in word_array if word.startswith('#')]).most_common(11)\n",
    "top_hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The #followfriday and #ff hashtags are the same. The other four hashtags that we'll try to predict are #squarespace, #iranelection, #musicmonday and #asot400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#ff              2985\n",
       "#squarespace      822\n",
       "#iranelection     436\n",
       "#musicmonday      387\n",
       "#asot400          316\n",
       "Name: hashtags, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiante a list of top hashtags while removing those that are too generic\n",
    "top_tags = ['#ff', '#squarespace', '#iranelection', '#musicmonday', '#asot400']\n",
    "\n",
    "# Copy tweeted hashtags (when they exist) to a separate column\n",
    "twDF['hashtags'] = twDF.text.str.findall(r'(?:(?<=\\s)|(?<=^))#.*?(?=\\s|$)')\n",
    "\n",
    "# Extract the first hashtag from the lists of hashtags\n",
    "twDF.hashtags = twDF.hashtags.apply(lambda x: x[0] if x!=[] else '')\n",
    "\n",
    "# Convert #followfriday tags to #ff, as they are the same\n",
    "twDF.hashtags = twDF.hashtags.apply(lambda x: '#ff' if x == '#followfriday' else x)\n",
    "\n",
    "# Filter a dataframe with only tweets with the top hashtags\n",
    "twDF = twDF[twDF.hashtags.isin(top_tags)][['text','hashtags', 'splitted']]\n",
    "twDF.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Inspect the frequency of the most common hashtags\n",
    "twDF.hashtags.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create a separate feature containing the tweets **without their hashtags**, and another feature with their numerical classes:\n",
    "\n",
    "1. #asot400 \n",
    "2. #musicmonday\n",
    "3. #ff\n",
    "4. #squarespace\n",
    "5. #iranelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>splitted</th>\n",
       "      <th>unhashed</th>\n",
       "      <th>hash_classes</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i wish i could watch the video feed...but the buffering sucks!  #asot400</td>\n",
       "      <td>#asot400</td>\n",
       "      <td>[i, wish, i, could, watch, the, video, feed...but, the, buffering, sucks!, #asot400]</td>\n",
       "      <td>i wish i could watch the video feed...but the buffering sucks!</td>\n",
       "      <td>1</td>\n",
       "      <td>(i, wish, i, could, watch, the, video, feed, ..., but, the, buffering, sucks, !)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       text  \\\n",
       "0  i wish i could watch the video feed...but the buffering sucks!  #asot400   \n",
       "\n",
       "   hashtags  \\\n",
       "0  #asot400   \n",
       "\n",
       "                                                                               splitted  \\\n",
       "0  [i, wish, i, could, watch, the, video, feed...but, the, buffering, sucks!, #asot400]   \n",
       "\n",
       "                                                           unhashed  \\\n",
       "0  i wish i could watch the video feed...but the buffering sucks!     \n",
       "\n",
       "   hash_classes  \\\n",
       "0             1   \n",
       "\n",
       "                                                                          tokenized  \n",
       "0  (i, wish, i, could, watch, the, video, feed, ..., but, the, buffering, sucks, !)  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the hashtags from the tweets and store the remaining words in a separate column\n",
    "twDF['unhashed'] = twDF.text.replace(to_replace=r'(?:(?<=\\s)|(?<=^))#.*?(?=\\s|$)',\n",
    "                                                     value='', regex=True)\n",
    "\n",
    "# Create a column with numerical hashtag classes\n",
    "twDF['hash_classes'] = pd.factorize(twDF['hashtags'])[0] + 1\n",
    "\n",
    "# Remove newlines and other extra whitespace by splitting and rejoining\n",
    "twDF['tokenized'] = twDF.unhashed.apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# Create SpaCy tokens from words\n",
    "twDF['tokenized'] = twDF.tokenized.apply(lambda x: nlp(x))\n",
    "twDF.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our resulting dataset has nearly five thousand tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4946, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words and parts of speech model\n",
    "\n",
    "Words are converted to SpaCy tokens to create the bag-of-words model with the 800 most common words. SpaCy helps us filter punctuation marks and stop words, which don't carry much information. Also, words are converted to their \"lemmas\" (or stem words) before they're counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an array of tokens from all tweets\n",
    "tokenlist = []\n",
    "twDF.tokenized.apply(lambda x: [tokenlist.append(i) for i in x])\n",
    "token_array = np.asarray(tokenlist)\n",
    "del tokenlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank', 'be', 'love', 'not', 'follow']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a counter dictionary with the most common words\n",
    "top_words = Counter([token.lemma_ for token in token_array\n",
    "             if not token.is_punct\n",
    "             and not token.is_stop]).most_common(800)\n",
    "\n",
    "# Extract the most common words into a list\n",
    "common_words = [item[0] for item in top_words]\n",
    "common_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 800\n",
      "Processing row 1600\n",
      "Processing row 2400\n",
      "Processing row 3200\n",
      "Processing row 4000\n",
      "Processing row 4800\n"
     ]
    }
   ],
   "source": [
    "# Create columns for every top word\n",
    "for i in common_words:\n",
    "    twDF[i] = 0\n",
    "\n",
    "# Create wordcount features in the data frame\n",
    "# Process each row, counting the occurrence of words in each tweet\n",
    "for i, sentence in enumerate(twDF.tokenized):\n",
    "\n",
    "    # Convert the sentence to lemmas, then filter out punctuation, stop words, and uncommon words\n",
    "    words = [token.lemma_\n",
    "             for token in sentence\n",
    "             if (not token.is_punct\n",
    "                 and not token.is_stop\n",
    "                 and token.lemma_ in common_words)]\n",
    "\n",
    "    # Populate the row with word counts\n",
    "    for word in words:\n",
    "        twDF.loc[i, word] += 1\n",
    "\n",
    "    # This counter is just to make sure the kernel didn't hang\n",
    "    if i % 800 == 0:\n",
    "        print(\"Processing row {}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create a series of features to count the number of \"Parts of Speech\" in the document (e.g., the number of nouns, verbs, numbers, etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add columns with empty values for parts of speech types\n",
    "pos_types = ['PROPN', 'ADV', 'NOUN', 'ADJ', 'VERB', 'CCONJ', 'PRON', 'NUM',\n",
    "        'X', 'INTJ', 'DET', 'ADP', 'PUNCT', 'PART', 'SYM', 'SPACE']\n",
    "for i in pos_types:\n",
    "    twDF[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 800\n",
      "Processing row 1600\n",
      "Processing row 2400\n",
      "Processing row 3200\n",
      "Processing row 4000\n",
      "Processing row 4800\n"
     ]
    }
   ],
   "source": [
    "# Create POS count features in the data frame\n",
    "# Process each row, counting the occurrence of parts of speech in each tweet\n",
    "for i, sentence in enumerate(twDF.tokenized):\n",
    "\n",
    "    # Convert the sentence to lemmas, then filter out punctuation, stop words, and uncommon words\n",
    "    POSs = [token.pos_ for token in sentence]\n",
    "\n",
    "    # Populate the row with word counts\n",
    "    for POS in POSs:\n",
    "        twDF.loc[i, POS] += 1\n",
    "\n",
    "    # This counter is just to make sure the kernel didn't hang\n",
    "    if i % 800 == 0:\n",
    "        print(\"Processing row {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>splitted</th>\n",
       "      <th>unhashed</th>\n",
       "      <th>hash_classes</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>thank</th>\n",
       "      <th>be</th>\n",
       "      <th>love</th>\n",
       "      <th>not</th>\n",
       "      <th>...</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NUM</th>\n",
       "      <th>X</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADP</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>PART</th>\n",
       "      <th>SYM</th>\n",
       "      <th>SPACE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i wish i could watch the video feed...but the buffering sucks!  #asot400</td>\n",
       "      <td>#asot400</td>\n",
       "      <td>[i, wish, i, could, watch, the, video, feed...but, the, buffering, sucks!, #asot400]</td>\n",
       "      <td>i wish i could watch the video feed...but the buffering sucks!</td>\n",
       "      <td>1</td>\n",
       "      <td>(i, wish, i, could, watch, the, video, feed, ..., but, the, buffering, sucks, !)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 822 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       text  \\\n",
       "0  i wish i could watch the video feed...but the buffering sucks!  #asot400   \n",
       "\n",
       "   hashtags  \\\n",
       "0  #asot400   \n",
       "\n",
       "                                                                               splitted  \\\n",
       "0  [i, wish, i, could, watch, the, video, feed...but, the, buffering, sucks!, #asot400]   \n",
       "\n",
       "                                                           unhashed  \\\n",
       "0  i wish i could watch the video feed...but the buffering sucks!     \n",
       "\n",
       "   hash_classes  \\\n",
       "0             1   \n",
       "\n",
       "                                                                          tokenized  \\\n",
       "0  (i, wish, i, could, watch, the, video, feed, ..., but, the, buffering, sucks, !)   \n",
       "\n",
       "   thank  be  love  not  ...    PRON  NUM  X  INTJ  DET  ADP  PUNCT  PART  \\\n",
       "0      0   0     0    0  ...       2    0  0     0    2    0      2     0   \n",
       "\n",
       "   SYM  SPACE  \n",
       "0    0      0  \n",
       "\n",
       "[1 rows x 822 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twDF.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model has 822 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4946, 822)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the BOW + POS model\n",
    "\n",
    "For our first attempt in predicting hashtags, we'll run a few supervised machine learning techniques to predict hashtags:\n",
    "\n",
    "- Random Forest\n",
    "- Logistic Regression\n",
    "- K-Nearest Neighbors\n",
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data set to train and test samples\n",
    "Y = twDF.hash_classes\n",
    "X = twDF.iloc[:,6:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9871924502864846\n",
      "\n",
      "Cross validation test scores: [0.70953101 0.71363636 0.69604863]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Random Forest Classifier model\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8955173576002696\n",
      "\n",
      "Cross validation test scores: [0.75189107 0.74393939 0.74620061]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Logistic Regression model\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8500168520390967\n",
      "\n",
      "Cross validation test scores: [0.7397882  0.73636364 0.70364742]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Gradient Boosting Classifier model\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.6521739130434783\n",
      "\n",
      "Cross validation test scores: [0.61270802 0.60909091 0.60334347]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Nearest Neighbor model\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "train = knn.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', knn.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning\n",
    "\n",
    "We'll tune the top scoring model (in this case, Logistic Regression) by randomly testing several parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'solver': 'liblinear', 'warm_start': True}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameter combinations\n",
    "params = {'C': [.1, 1, 10],\n",
    "          'solver': ['liblinear', 'newton-cg', 'saga'],\n",
    "          'warm_start': [True, False]}\n",
    "\n",
    "# Instantiate RandomizedSearchCV to test all possible parameter combinations \n",
    "random_CV = RandomizedSearchCV(lr, params, n_iter=18, cv=3, scoring='f1_micro')\n",
    "\n",
    "# Run RandomizedSearchCV\n",
    "random_CV.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the best parameter combination\n",
    "random_CV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the ideal parameters are also the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8955173576002696\n",
      "\n",
      "Cross validation test scores: [0.75189107 0.74393939 0.74620061]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Logistic Regression model\n",
    "lr = LogisticRegression(C=1, solver='liblinear', warm_start=True)\n",
    "train = lr.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have five classes, we'll generate a confusion matrix to visualize whether hashtags have been consistently predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix | Logistic Regression Predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>hash_classes</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>73</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>74</td>\n",
       "      <td>1136</td>\n",
       "      <td>86</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>31</td>\n",
       "      <td>213</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "hash_classes   1   2     3    4   5\n",
       "row_0                              \n",
       "1             42   3     3    7   7\n",
       "2             12  73     9    4   2\n",
       "3             52  74  1136   86  48\n",
       "4             21  18    31  213  17\n",
       "5              5   4     6   10  96"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test and inspect the results from the top performing model\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "ct = pd.crosstab(y_pred, y_test)\n",
    "\n",
    "print('Confusion Matrix | Logistic Regression Predictions')\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also view selected classification scoring methods:\n",
    "\n",
    "- **Precision**: The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is the ability of the classifier not to misclassify samples of each specific class. The best value is 1 and the worst value is 0.\n",
    "\n",
    "\n",
    "- **Recall**: The recall is the ratio tp / (tp + fn). The recall is intuitively the ability of the classifier to correctly predict each class. The best value is 1 and the worst value is 0.\n",
    "\n",
    "\n",
    "- **f1-Score**: The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "In the multi-class and multi-label case, this is the weighted average of the F1 score of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.68      0.32      0.43       132\n",
      "          2       0.73      0.42      0.54       172\n",
      "          3       0.81      0.96      0.88      1185\n",
      "          4       0.71      0.67      0.69       320\n",
      "          5       0.79      0.56      0.66       170\n",
      "\n",
      "avg / total       0.78      0.79      0.77      1979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model was able to adequately predict the two classes with the greatest number of samples (class3 = #ff | class4 = #squarespace).\n",
    "\n",
    "The model had low recall scores for the two classes with the least number of observations (in other words, it over-predicted those classes).\n",
    "\n",
    "## Clustering with BOW\n",
    "\n",
    "We'll now use three clustering methods and see if they can group tweets with the same hashtags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "\n",
    "K-means is a method to cluster data points with similar variances. The algorithm tries to choose means (called centroids) that minimize inertia. The formula for inertia is:\n",
    "\n",
    "$\\sum(\\bar{x}_c - x_i)^2$\n",
    "\n",
    "Inertia is the sum of the squared differences between the centroid of a cluster (the mean $\\bar{x}_c$) and the data points in the cluster ($x_i$).  The goal is to define cluster means so that the distance between a cluster mean and all the data points within the cluster is as small as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate and fit the K-Means clustering model\n",
    "y_pred = KMeans(n_clusters=5).fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix | Logistic Regression Predictions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>hash_classes</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110</td>\n",
       "      <td>96</td>\n",
       "      <td>880</td>\n",
       "      <td>244</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>34</td>\n",
       "      <td>305</td>\n",
       "      <td>156</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>56</td>\n",
       "      <td>440</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>173</td>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "hash_classes    1   2    3    4   5\n",
       "row_0                              \n",
       "0             110  96  880  244  83\n",
       "1              27  34  305  156  90\n",
       "2               0   0    2    0   0\n",
       "3              25  56  440   69  70\n",
       "4              22  29  173   33  23"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "ct = pd.crosstab(y_pred, y_train)\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's really no consistency between k-means clusters and hashtags. \n",
    "\n",
    "### Mean Shift\n",
    "\n",
    "Mean shift is a clustering method that works by first calculating the probability that a data point will be present at any point in the n-dimensional space defined by the number of features. The surface of probabilities is called a kernel density surface.\n",
    "\n",
    "A kernel function $K(x_i - x)$ is used to determine the weight of nearby points. The weighted mean of the density in the window determined by K is:\n",
    "\n",
    "<img src='Screen Shot 2018-07-23 at 9.22.43 PM.png' width=250>\n",
    "\n",
    "Mean-shift is an iterative algorithm. At each iteration, each data point is shifted toward the nearest group of points (or cluster means). If a data point is already closest to its cluster mean, it stays where it is.\n",
    "\n",
    "Once all data points have reached the point where they are at their nearest mean, and all further shifts (if any) are smaller than a given threshold, the algorithm terminates. The data points are then assigned a \"cluster\" based on their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimated clusters: 9\n"
     ]
    }
   ],
   "source": [
    "# Declare and fit the model\n",
    "ms = MeanShift(bin_seeding=True)\n",
    "ms.fit(X_train)\n",
    "\n",
    "# Extract cluster assignments for each data point\n",
    "labels = ms.labels_\n",
    "\n",
    "# Coordinates of the cluster centers\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "# Count our clusters\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "print(\"Number of estimated clusters: {}\".format(n_clusters_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>hash_classes</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>158</td>\n",
       "      <td>1134</td>\n",
       "      <td>292</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "hash_classes    1    2     3    4    5\n",
       "row_0                                 \n",
       "0             128  158  1134  292  154\n",
       "1               0    2     6    2    3\n",
       "3               2    4    17   13    6\n",
       "4               1    2     6    6    3\n",
       "5               1    6    21    7    4\n",
       "6               0    0     1    0    0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ms.predict(X_test)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "ct = pd.crosstab(y_pred, y_test)\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean shift divided the test data set into seven clusters, grouping most values into one large cluster.\n",
    "\n",
    "### Affinity Propagation\n",
    "\n",
    "Affinity Propagation is based on defining exemplars for data points. An exemplar is a data point similar enough to another data point that one could conceivably be represented by the other. Similarity between points is interpreted to mean how well-suited a given data point is to be an exemplar of data another point (and vice-versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 157\n"
     ]
    }
   ],
   "source": [
    "# Declare the model and fit it in one statement\n",
    "af = AffinityPropagation().fit(X_train)\n",
    "\n",
    "# Pull the number of clusters and cluster assignments for each data point\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "labels = af.labels_\n",
    "\n",
    "print('Estimated number of clusters: {}'.format(n_clusters_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affinity propagation wrongly estimated a much larger number of clusters and was uncapable of grouping tweets with the same hashtag.\n",
    "\n",
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "Similar to PCA, SVD is a mathematical method that is used to reduce the number of dimensions (or features) in a model.\n",
    "\n",
    "We'll reduce the number of features from 800 to 150, and run the Gradient Boosting model again to see if it brings a considerable change in model accuracy, and especially if it helps improve our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 94.87921970404707\n"
     ]
    }
   ],
   "source": [
    "# Reduce the number of variables with SVD\n",
    "svd= TruncatedSVD(150)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "# Declare and fit SVD\n",
    "X_lsa = lsa.fit_transform(twDF.iloc[:,6:])\n",
    "\n",
    "# Inspect the variance explained by the resulting SVD components\n",
    "variance_explained = svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame with all features: (4946, 822)\n",
      "Data frame after SVD: (4946, 150)\n"
     ]
    }
   ],
   "source": [
    "# Preview the shape of the current data frame\n",
    "print('Data frame with all features:', twDF.shape)\n",
    "\n",
    "# Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "df_lsa = pd.DataFrame(X_lsa, index=[twDF.hashtags, twDF.hash_classes])\n",
    "\n",
    "# Inspect the resulting dataframe shape\n",
    "print('Data frame after SVD:', df_lsa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data set to train and test samples\n",
    "Y = twDF.hash_classes\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_lsa, Y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9976407145264578\n",
      "\n",
      "Cross validation test scores: [0.71860817 0.7030303  0.7112462 ]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Gradient Boosting Classifier model\n",
    "clf = ensemble.GradientBoostingClassifier(loss='deviance', n_estimators=100, warm_start=True,\n",
    "                                         max_depth=5)\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing classes:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.36      0.16      0.22       132\n",
      "          2       0.72      0.28      0.41       172\n",
      "          3       0.76      0.96      0.85      1185\n",
      "          4       0.74      0.61      0.67       320\n",
      "          5       0.79      0.40      0.53       170\n",
      "\n",
      "avg / total       0.73      0.74      0.71      1979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the solution against the data.\n",
    "print('Comparing classes:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, SVD was able to reduce the number of dimensions in our model without losing much variance. The accuracy of gradient boosting predictions dropped by only approximately two percentage points.\n",
    "\n",
    "### Clustering with SVD\n",
    "\n",
    "Now let's see what matters most - if SVD helps improve our clusters. We'll also do some parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "\n",
    "We've changed the *n_init* parameter value to increase the number of times the k-means algorithm is run with different centroid seeds. The model then chooses the output with the best result in terms of inertia, which is a mathematical method to measure information gain. The number of model iterations was increased too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare and fit the model\n",
    "y_pred = KMeans(n_clusters=5, n_init=200, max_iter=1500).fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>hash_classes</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>53</td>\n",
       "      <td>497</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>35</td>\n",
       "      <td>307</td>\n",
       "      <td>198</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>51</td>\n",
       "      <td>308</td>\n",
       "      <td>70</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>43</td>\n",
       "      <td>301</td>\n",
       "      <td>67</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>33</td>\n",
       "      <td>387</td>\n",
       "      <td>135</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "hash_classes   1   2    3    4   5\n",
       "row_0                             \n",
       "0             16  53  497   32  50\n",
       "1             49  35  307  198  79\n",
       "2             42  51  308   70  38\n",
       "3             26  43  301   67  38\n",
       "4             51  33  387  135  61"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "ct = pd.crosstab(y_pred, y_train)\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>hash_classes</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>29</td>\n",
       "      <td>376</td>\n",
       "      <td>116</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>52</td>\n",
       "      <td>310</td>\n",
       "      <td>71</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>42</td>\n",
       "      <td>312</td>\n",
       "      <td>74</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>53</td>\n",
       "      <td>487</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>39</td>\n",
       "      <td>315</td>\n",
       "      <td>211</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "hash_classes   1   2    3    4   5\n",
       "row_0                             \n",
       "0             48  29  376  116  56\n",
       "1             42  52  310   71  38\n",
       "2             28  42  312   74  39\n",
       "3             14  53  487   30  50\n",
       "4             52  39  315  211  83"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "ct = pd.crosstab(y_pred, y_train)\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the reduced number of variables and model parameter tuning, K-Means continues to be unable to distinguish clusters with similar hashtags.\n",
    "\n",
    "### Mean Shift\n",
    "There aren't many parameters to tune in Mean Shift. We'll let the model estimate the bandwidth using the *sklearn.cluster.estimate_bandwidth* method (which is the default). When *bin_seeding* is set to \"False\", the model takes the locations of all points when defining the initial locations of cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimated clusters: 5\n"
     ]
    }
   ],
   "source": [
    "# Declare and fit the model.\n",
    "ms = MeanShift(bin_seeding=False)\n",
    "y_pred = ms.fit_predict(X_train)\n",
    "\n",
    "# Extract cluster assignments for each data point.\n",
    "labels = ms.labels_\n",
    "\n",
    "# Coordinates of the cluster centers.\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "# Count our clusters.\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "print(\"Number of estimated clusters: {}\".format(n_clusters_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>hash_classes</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>172</td>\n",
       "      <td>211</td>\n",
       "      <td>1767</td>\n",
       "      <td>490</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "hash_classes    1    2     3    4    5\n",
       "row_0                                 \n",
       "0             172  211  1767  490  265\n",
       "1               7    4    22    7    1\n",
       "2               2    0     4    2    0\n",
       "3               1    0     6    0    0\n",
       "4               2    0     1    3    0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "ct = pd.crosstab(y_pred, y_train)\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimated clusters: 4\n"
     ]
    }
   ],
   "source": [
    "# Declare and fit the model.\n",
    "ms = MeanShift(bin_seeding=True)\n",
    "y_pred = ms.fit_predict(X_train)\n",
    "\n",
    "# Extract cluster assignments for each data point.\n",
    "labels = ms.labels_\n",
    "\n",
    "# Coordinates of the cluster centers.\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "# Count our clusters.\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "print(\"Number of estimated clusters: {}\".format(n_clusters_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, Mean Shift grouped most samples into a single, large cluster, while other clusters remained with just a very small portion of samples.\n",
    "\n",
    "### Spectral Clustering\n",
    "\n",
    "Spectral Clustering is based on quantifying similarity between data points. The method defines a similarity matrix of n x n dimensions, where n is the number of data points in the dataset. The matrix is made up of indices of similarity for every pairwise combination of data points. Then, a transformation matrix is applied to calculate a set of eigenvectors with appropriate eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare and fit the model.\n",
    "sc = SpectralClustering(n_clusters=5, affinity='rbf', gamma=3)\n",
    "y_pred = sc.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>hash_classes</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>367</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "      <td>377</td>\n",
       "      <td>96</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>59</td>\n",
       "      <td>292</td>\n",
       "      <td>54</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>53</td>\n",
       "      <td>310</td>\n",
       "      <td>69</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81</td>\n",
       "      <td>54</td>\n",
       "      <td>454</td>\n",
       "      <td>272</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "hash_classes   1   2    3    4    5\n",
       "row_0                              \n",
       "0              6   4  367   11    5\n",
       "1             30  45  377   96   56\n",
       "2             28  59  292   54   65\n",
       "3             39  53  310   69   33\n",
       "4             81  54  454  272  107"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a confusion matrix\n",
    "ct = pd.crosstab(y_pred, y_train)\n",
    "\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several parameter combinations were tested, including:\n",
    "\n",
    "- {affinity: rbf} and {gamma: [.1, 1, 5, 10]}\n",
    "- {affinity: nearest_neighbor} and {n_neighbors: [10, 30, 50, 100]}\n",
    "\n",
    "Spectral clustering was also unable to create clusters with the same hashtags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Model\n",
    "\n",
    "TF-IDF creates unique weights for each sentence that combine the *term frequency* (how often a word appears within an individual document) and the *IDF* (which gives more weight to words that occur less often).\n",
    "\n",
    "The tf_idf score will be highest for a term that occurs a lot within a small number of sentences, and lowest for a word that occurs in most or all sentences.\n",
    "\n",
    "We'll tune the TF-IDF model and run our supervised machine learning models to predict hashtags. We'll then attempt to create clusters and see if they perform better with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate Sklearn's TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=.7, # drop words that occur in more than 70% of the paragraphs\n",
    "                             min_df=5, # only use words that appear at least five times\n",
    "                             stop_words='english',\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "#Applying the vectorizer\n",
    "vectorized = vectorizer.fit_transform(twDF.unhashed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting into training and test sets\n",
    "Y = twDF.hash_classes\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized, Y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from TfidfVectorizer are stored in a compressed sparse row format where for each row in the data frame, the column index of the non-zero values are represented in a tuple, followed by the non-zero value itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 50)\t0.8017848761672317\n",
      "  (0, 60)\t0.5976127612003419\n"
     ]
    }
   ],
   "source": [
    "# Inspect how the data is stored in Compressed Sparse Row Format\n",
    "for i in X_train[4]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF and supervised machine learning to predict Twitter hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8257499157398045\n",
      "\n",
      "Cross validation test scores: [0.70347958 0.69545455 0.70972644]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Logistic Regression model\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.6215032018874284\n",
      "\n",
      "Cross validation test scores: [0.60060514 0.6030303  0.60942249]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Nearest Neighbor model\n",
    "knn = KNeighborsClassifier(n_neighbors=30)\n",
    "train = knn.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', knn.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9605662285136501\n",
      "\n",
      "Cross validation test scores: [0.72314675 0.71212121 0.72340426]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Random Forest Classifier model\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8446241995281429\n",
      "\n",
      "Cross validation test scores: [0.73373676 0.71666667 0.7218845 ]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Gradient Boosting Classifier model\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(clf, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'n_estimators': 100, 'warm_start': True}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parameter combinations\n",
    "params = {'n_estimators': [100, 200],\n",
    "          'criterion': ['gini', 'entropy'],\n",
    "          'warm_start': [True, False]}\n",
    "\n",
    "# Instantiate RandomizedSearchCV to test all possible parameter combinations \n",
    "random_CV = RandomizedSearchCV(rfc, params, n_iter=8, cv=3, scoring='f1_micro')\n",
    "\n",
    "# Run RandomizedSearchCV\n",
    "random_CV.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the best parameter combination\n",
    "random_CV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9686552072800809\n",
      "\n",
      "Cross validation test scores: [0.75037821 0.6969697  0.72796353]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Random Forest Classifier model\n",
    "rfc = ensemble.RandomForestClassifier(criterion='entropy', n_estimators=200, warm_start=False)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.80      0.30      0.43       132\n",
      "          2       0.81      0.31      0.45       172\n",
      "          3       0.73      0.98      0.84      1185\n",
      "          4       0.85      0.46      0.60       320\n",
      "          5       0.88      0.47      0.61       170\n",
      "\n",
      "avg / total       0.77      0.75      0.72      1979\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualize selected classification metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our supervised machine learning models had a similar result with the *TF-IDF* and the *Bag of Words* features. The latter still had a slightly higher performance.\n",
    "\n",
    "Now let's see how our clusters come out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare and fit the model\n",
    "y_pred = KMeans(n_clusters=5).fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means clusters against the data:\n",
      "hash_classes    1    2     3    4    5\n",
      "row_0                                 \n",
      "0             168  167  1020  490  255\n",
      "1               3    3   365    3    1\n",
      "2               6    2   196    0    0\n",
      "3               6   42    28    9   10\n",
      "4               1    1   191    0    0\n"
     ]
    }
   ],
   "source": [
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better, but still not good enough. Cluster zero was able to match most of class 3, but approximately half of the samples from this class are still scattered among other clusters.\n",
    "\n",
    "## Clustering based on TF-IDF Cosine Similarity\n",
    "\n",
    "Our last clustering attempt will be with Cosine Similarity.\n",
    "\n",
    "We'll use Sklearn's *cosine_similarity* method to calculate the cosine of the vectors of each tweet, and create a similarity matrix where each row has the cosine values of the row's tweet, and all other tweets.\n",
    "\n",
    "The cosine similarity of two vectors is a number between 0 and 1.0 where a value of 1.0 means the two vectors are exactly the same.\n",
    "\n",
    "Since we have 4946 rows, our similarity matrix will have the same number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4936</th>\n",
       "      <th>4937</th>\n",
       "      <th>4938</th>\n",
       "      <th>4939</th>\n",
       "      <th>4940</th>\n",
       "      <th>4941</th>\n",
       "      <th>4942</th>\n",
       "      <th>4943</th>\n",
       "      <th>4944</th>\n",
       "      <th>4945</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250451</td>\n",
       "      <td>0.36547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.732314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.265646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 4946 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4         5     6     7         8        9     \\\n",
       "0   1.0   0.0   0.0   0.0   0.0  0.000000   0.0   0.0  0.250451  0.36547   \n",
       "1   0.0   1.0   0.0   0.0   0.0  0.732314   0.0   0.0  0.000000  0.00000   \n",
       "2   0.0   0.0   1.0   0.0   0.0  0.000000   0.0   0.0  0.000000  0.00000   \n",
       "\n",
       "   ...   4936  4937      4938      4939  4940  4941  4942  4943  4944  4945  \n",
       "0  ...    0.0   0.0  0.000000  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1  ...    0.0   0.0  0.265646  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2  ...    0.0   0.0  0.000000  0.181986   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[3 rows x 4946 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a cosine similarity matrix and store it in a dataframe\n",
    "sim_mtx = pd.DataFrame(data = cosine_similarity(vectorized))\n",
    "\n",
    "sim_mtx.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare and fit the clustering model\n",
    "cos_sim_clusters = KMeans(n_clusters=5).fit_predict(sim_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the assigned categories to the ones in the data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>hash_classes</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>280</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>229</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>299</td>\n",
       "      <td>381</td>\n",
       "      <td>1760</td>\n",
       "      <td>814</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>647</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "hash_classes    1    2     3    4    5\n",
       "row_0                                 \n",
       "0               4    1   280    2    0\n",
       "1               7    1   229    1    0\n",
       "2             299  381  1760  814  434\n",
       "3               5    3   647    5    2\n",
       "4               1    1    69    0    0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a confusion matrix to inspect the results\n",
    "print('Comparing the assigned categories to the ones in the data:')\n",
    "pd.crosstab(cos_sim_clusters, twDF.hash_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've ran out of options and haven't been able to generate useful clusters with unsupervised methods.\n",
    "\n",
    "## Assigning hashtags based on Cosine Similarity\n",
    "\n",
    "Nevertheless, as a final experiment, we'll see if tweets with high similarity also have the same hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary where the key is the ordered row index, and the value is the index of the\n",
    "# tweet with the highest cosine value (in other words, the most similar tweet)\n",
    "most_similar = {}\n",
    "for i in (sim_mtx):\n",
    "    most_similar[i] = sim_mtx[i].nlargest(2).index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a list with the hash classes of the most similar Tweets\n",
    "y_similar = []\n",
    "for i in most_similar.values():\n",
    "    y_similar.append(twDF.loc[i,'hash_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.22      0.48      0.30       316\n",
      "          2       0.43      0.38      0.41       387\n",
      "          3       0.83      0.76      0.79      2985\n",
      "          4       0.56      0.51      0.54       822\n",
      "          5       0.54      0.53      0.54       436\n",
      "\n",
      "avg / total       0.69      0.65      0.67      4946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check how often the original tweet and its most similar tweet have the same hashtag\n",
    "print(classification_report(twDF.hash_classes, y_similar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that cosine similarity is effective to predict at least the #ff hashtag. Moving forward, we can group together tweets with high cosine similarities (e.g., values above 0.7) and see if hashtags are consistent among groups.\n",
    "\n",
    "### Visualizing most similar tweets\n",
    "\n",
    "As a convenience, we'll create a DataFrame to visualize the most similar tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe - assign column names\n",
    "sim_df = pd.DataFrame(columns=['original_tweet','most_similar'])\n",
    "\n",
    "# Copy the original tweets to our new dataframe\n",
    "sim_df['original_tweet'] = twDF.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now copy the \"original tweets'\" most similar tweets\n",
    "for key, value in most_similar.items():\n",
    "    sim_df.loc[key, 'most_similar'] = twDF.loc[value, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_tweet</th>\n",
       "      <th>most_similar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i wish i could watch the video feed...but the buffering sucks!  #asot400</td>\n",
       "      <td>my video feed went down again..   #asot400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@donaxvariabilis omt love you! opening it in a new tab now. im suckered i missed daniel kandi!!  #asot400</td>\n",
       "      <td>@bobbymonkz yup replacing daniel kandi since he couldnt get a passport unfortunatly  #asot400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ouch following the #asot400 in tweetdeck exceeded my tweet limit!</td>\n",
       "      <td>@jprigent i am following your sis  #followfriday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i'm gonna be so sad when this is over  #asot400</td>\n",
       "      <td>gonna do this again: #musicmonday craigslist by @alyankovic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>want mooooooooore      #asot400</td>\n",
       "      <td>i want beer now  #squarespace</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              original_tweet  \\\n",
       "0                                   i wish i could watch the video feed...but the buffering sucks!  #asot400   \n",
       "1  @donaxvariabilis omt love you! opening it in a new tab now. im suckered i missed daniel kandi!!  #asot400   \n",
       "2                                         ouch following the #asot400 in tweetdeck exceeded my tweet limit!    \n",
       "3                                                            i'm gonna be so sad when this is over  #asot400   \n",
       "4                                                                            want mooooooooore      #asot400   \n",
       "\n",
       "                                                                                    most_similar  \n",
       "0                                                     my video feed went down again..   #asot400  \n",
       "1  @bobbymonkz yup replacing daniel kandi since he couldnt get a passport unfortunatly  #asot400  \n",
       "2                                               @jprigent i am following your sis  #followfriday  \n",
       "3                                   gonna do this again: #musicmonday craigslist by @alyankovic   \n",
       "4                                                                  i want beer now  #squarespace  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It's hard to say exactly why the clustering methods had such low performance with the Twitter data set. Among all attempts, clustering with TF-IDF has proven to be the most effective, but still inadequate.\n",
    "\n",
    "In the cells below, we compare the proportion of stopwords per tweet, against sentences in Alice in Wonderland. We notice that tweets have a smaller proportion of stopwords than common literature - only 33% of the words in a tweet are stopwords. This should mean that we have sufficient words to create and learn from our tweet vectors.\n",
    "\n",
    "Nevertheless, there are several issues that complicate matters. For one, our Twitter data set was written by hundreds of thousands of users. Tweets also have many punctuation marks, and unsername handles,  which were all  stripped from the input variables. Also, there are many misspellings, and the sentences generally aren't well formed.\n",
    "\n",
    "The supervised machine learning methods, on the other hand, were successful in predicting hashtags. The main reason is that we have labeled data to train the models, which is not the case with the unsupervised clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of stopwords per Tweet: 4.308329963606955\n",
      "Average number of words per Tweet: 12.812778002426203\n"
     ]
    }
   ],
   "source": [
    "# Create a list with the number of stopwords in each tweet\n",
    "stopword_count = []\n",
    "twDF.splitted.apply(\n",
    "    lambda x: stopword_count.append(sum([1 for w in x if w in stopwords.words(\"english\")]))\n",
    ")\n",
    "\n",
    "# Create a list with the number of words in each tweet\n",
    "word_count = []\n",
    "twDF.splitted.apply(\n",
    "    lambda x: word_count.append(sum([1 for w in x]))\n",
    ")\n",
    "\n",
    "# Inspect the average number of stopwords and words per tweet\n",
    "print('Average number of stopwords per Tweet:', np.mean(stopword_count))\n",
    "print('Average number of words per Tweet:', np.mean(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice in Wonderland has 1678 sentences.\n",
      "Proportion of stopwords in sentences in Alice in Wonderland: 0.46135105204872645\n",
      "Proportion of stopwords in Tweets: 0.33625260367354665\n"
     ]
    }
   ],
   "source": [
    "# Import the Alice In Wonderland text and perform some cleaning\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# This pattern matches all text between square brackets.\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "alice = re.sub(pattern, \"\", alice)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "alice = ' '.join(alice.split())\n",
    "alice_doc = nlp(alice)\n",
    "\n",
    "# Initial exploration of sentences.\n",
    "alice_sentences = list(alice_doc.sents)\n",
    "print(\"Alice in Wonderland has {} sentences.\".format(len(alice_sentences)))\n",
    "\n",
    "# Create a list with the number of stopwords in each sentence from Alice in Wonderland\n",
    "alice_stopword_count = []\n",
    "alice_stopword_count.append(\n",
    "    [[sum(1 for w in sent.text.split() if w.lower() in stopwords.words(\"english\"))] for sent in alice_sentences])\n",
    "\n",
    "# Create a list with the number of words in each sentence from Alice in Wonderland\n",
    "alice_word_count = [len(i.text.split()) for i in alice_sentences]\n",
    "\n",
    "# Inspect the average number of stopwords and words per sentence\n",
    "print('Proportion of stopwords in sentences in Alice in Wonderland:', np.mean(alice_stopword_count) / np.mean(alice_word_count))\n",
    "print('Proportion of stopwords in Tweets:', np.mean(stopword_count) / np.mean(word_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
