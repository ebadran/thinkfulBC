{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U4 L4 P5 - Build a NLP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "- Data cleaning / processing / language parsing\n",
    "- Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "- Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "- Assess your models using cross-validation and determine whether one model performed better.\n",
    "- Pick one of the models and try to increase accuracy by at least 5 percentage points.\n",
    "\n",
    "Write up your report in a Jupyter notebook. Be sure to explicitly justify the choices you make throughout, and submit it below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "We'll use a Kaggle data set consisting of 1.6 million tweets, labeled as [0 = negative, 2 = neutral, 4 = positive].\n",
    "\n",
    "More information about the data set can be found [here](http://help.sentiment140.com/for-students/). The original research paper can be found [here](https://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf).\n",
    "\n",
    "Citation: Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12.\n",
    "\n",
    "Link to download the data set:\n",
    "https://www.kaggle.com/kazanova/sentiment140/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the SpaCy module\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data set file into a data frame\n",
    "tweetsDf = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin1',\n",
    "                       names=['target','id','date','flag','user','text'])\n",
    "\n",
    "print(tweetsDf.shape)\n",
    "tweetsDf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Metadata\n",
    "\n",
    "- target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "- id: The id of the tweet\n",
    "- date: the date of the tweet\n",
    "- flag: The query (if there is no query, then this value is NO_QUERY)\n",
    "- user: the user that tweeted\n",
    "- text: the text of the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words and parts of speech model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract a smaller sample to reduce processing time\n",
    "tweetsDfhead = tweetsDf.sample(100000)\n",
    "tweetsDfhead.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove newlines and other extra whitespace by splitting and rejoining\n",
    "tweetsDfhead['tokenized'] = tweetsDfhead.text.apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# Create SpaCy tokens from words\n",
    "tweetsDfhead['tokenized'] = tweetsDfhead.tokenized.apply(lambda x: nlp(x))\n",
    "\n",
    "# Create a list of tokens from all tweets\n",
    "tokenlist = []\n",
    "tweetsDfhead.tokenized.apply(lambda x: [tokenlist.append(i) for i in x])\n",
    "\n",
    "# Convert token list to more efficient numpy array\n",
    "token_array = np.asarray(tokenlist)\n",
    "\n",
    "# Delete token list to free up memory\n",
    "del tokenlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-PRON-', 'be', 'not', 'go', 'good']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a counter dictionary with the most common words\n",
    "top_words = Counter([token.lemma_ for token in token_array\n",
    "             if not token.is_punct\n",
    "             and not token.is_stop]).most_common(50)\n",
    "\n",
    "# Extract the most common words into a list\n",
    "common_words = [item[0] for item in top_words]\n",
    "common_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create columns for every \n",
    "for i in common_words:\n",
    "    tweetsDfhead[i] = 0\n",
    "\n",
    "# Create wordcount features in the data frame\n",
    "# Process each row, counting the occurrence of words in each tweet\n",
    "for i, sentence in enumerate(tweetsDfhead.tokenized):\n",
    "\n",
    "    # Convert the sentence to lemmas, then filter out punctuation, stop words, and uncommon words\n",
    "    words = [token.lemma_\n",
    "             for token in sentence\n",
    "             if (not token.is_punct\n",
    "                 and not token.is_stop\n",
    "                 and token.lemma_ in common_words)]\n",
    "\n",
    "    # Populate the row with word counts\n",
    "    for word in words:\n",
    "        tweetsDfhead.loc[i, word] += 1\n",
    "\n",
    "    # This counter is just to make sure the kernel didn't hang\n",
    "    if i % 100 == 0:\n",
    "        print(\"Processing row {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add columns with empty values for parts of speech types\n",
    "pos_types = ['PROPN', 'ADV', 'NOUN', 'ADJ', 'VERB', 'CCONJ', 'PRON', 'NUM',\n",
    "        'X', 'INTJ', 'DET', 'ADP', 'PUNCT', 'PART', 'SYM', 'SPACE']\n",
    "\n",
    "for i in pos_types:\n",
    "    tweetsDfhead[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create POS count features in the data frame\n",
    "# Process each row, counting the occurrence of parts of speech in each tweet\n",
    "for i, sentence in enumerate(tweetsDfhead.tokenized):\n",
    "\n",
    "    # Convert the sentence to lemmas, then filter out punctuation, stop words, and uncommon words\n",
    "    POSs = [token.pos_ for token in sentence]\n",
    "\n",
    "    # Populate the row with word counts\n",
    "    for POS in POSs:\n",
    "        tweetsDfhead.loc[i, POS] += 1\n",
    "\n",
    "    # This counter is just to make sure the kernel didn't hang\n",
    "    if i % 100 == 0:\n",
    "        print(\"Processing row {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>-PRON-</th>\n",
       "      <th>be</th>\n",
       "      <th>not</th>\n",
       "      <th>...</th>\n",
       "      <th>PRON</th>\n",
       "      <th>NUM</th>\n",
       "      <th>X</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADP</th>\n",
       "      <th>PUNCT</th>\n",
       "      <th>PART</th>\n",
       "      <th>SYM</th>\n",
       "      <th>SPACE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1880773791</td>\n",
       "      <td>Fri May 22 02:27:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BenQIndia</td>\n",
       "      <td>@Netra &amp;quot;just twit&amp;quot; is a great motto..thanks! see you around</td>\n",
       "      <td>(@Netra, &amp;, quot;just, twit&amp;quot, ;, is, a, great, motto, .., thanks, !, see, you, around)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2030872846</td>\n",
       "      <td>Thu Jun 04 09:00:16 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>brooke_hyatt</td>\n",
       "      <td>Dead frogs smell amazing</td>\n",
       "      <td>(Dead, frogs, smell, amazing)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag          user  \\\n",
       "0       4  1880773791  Fri May 22 02:27:19 PDT 2009  NO_QUERY     BenQIndia   \n",
       "1       0  2030872846  Thu Jun 04 09:00:16 PDT 2009  NO_QUERY  brooke_hyatt   \n",
       "\n",
       "                                                                     text  \\\n",
       "0  @Netra &quot;just twit&quot; is a great motto..thanks! see you around    \n",
       "1                                               Dead frogs smell amazing    \n",
       "\n",
       "                                                                                    tokenized  \\\n",
       "0  (@Netra, &, quot;just, twit&quot, ;, is, a, great, motto, .., thanks, !, see, you, around)   \n",
       "1                                                               (Dead, frogs, smell, amazing)   \n",
       "\n",
       "   -PRON-  be  not  ...    PRON  NUM  X  INTJ  DET  ADP  PUNCT  PART  SYM  \\\n",
       "0       0   0    0  ...       1    0  0     0    1    0      3     0    0   \n",
       "1       0   0    0  ...       0    0  0     0    0    0      0     0    0   \n",
       "\n",
       "   SPACE  \n",
       "0      0  \n",
       "1      0  \n",
       "\n",
       "[2 rows x 73 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDfhead.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the BOW + POS model\n",
    "\n",
    "The Random Forest is the only model that is overfitting with the training data. Overall accuracy isn't great for any of the three models, but is above 62%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data set to train and test samples\n",
    "Y = tweetsDfhead.target\n",
    "X = tweetsDfhead.iloc[:,10:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9649333333333333\n",
      "\n",
      "Cross validation test scores: [0.62381881 0.62501875 0.62541254]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Random Forest Classifier model\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.6636166666666666\n",
      "\n",
      "Cross validation test scores: [0.66334183 0.66101695 0.66179118]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Logistic Regression model\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.6649166666666667\n",
      "\n",
      "Cross validation test scores: [0.65816709 0.65921704 0.66171617]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Gradient Boosting Classifier model\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the BOW only model\n",
    "While the Random Forest Classifier is no longer overfitting, the test performance on all models is slightly lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-generate the train and test samples and run the model with BOW features only\n",
    "Y = tweetsDfhead.target\n",
    "X = tweetsDfhead.iloc[:,10:-16]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.7212833333333334\n",
      "\n",
      "Cross validation test scores: [0.62906855 0.62081896 0.63133813]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Random Forest Classifier model\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.64765\n",
      "\n",
      "Cross validation test scores: [0.64369282 0.64354282 0.64393939]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Logistic Regression model\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.6460833333333333\n",
      "\n",
      "Cross validation test scores: [0.64039298 0.63971801 0.64528953]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Gradient Boosting Classifier model\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Model\n",
    "The TF-IDF model is much more computationally intensive and has a lower performance with Gradient Boosting.\n",
    "\n",
    "Also, both Random Forest and Logistic Regression are overfitting with the training dataset.\n",
    "\n",
    "That said, TF-IDF was able to improve Logistic Regression by ~1 percentage point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate Sklearn's DF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=.005, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=1, # only use words that appear at least once\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "#Applying the vectorizer\n",
    "vectorized = vectorizer.fit_transform(tweetsDfhead.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting into training and test sets\n",
    "Y = tweetsDfhead.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized, Y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9703666666666667\n",
      "\n",
      "Cross validation test scores: [0.62614369 0.62824359 0.6340384 ]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Random Forest Classifier model\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8296666666666667\n",
      "\n",
      "Cross validation test scores: [0.67054147 0.66701665 0.66884188]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Logistic Regression model\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.5930833333333333\n",
      "\n",
      "Cross validation test scores: [0.58902055 0.58332083 0.58378338]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the Gradient Boosting Classifier model\n",
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "# Inspect the results\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nCross validation test scores:', cross_val_score(train, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "While the top score was the TF-IDF model with Logistic Regression, its performance was only ~1% better than the \"Bag of Words + Parts of Speech\" model, but at a much higher computational cost.\n",
    "\n",
    "Next steps:\n",
    "\n",
    "- Tune the Bag of Words + Parts of Speech\" model parameters and try to increase accuracy by at least 5 percentage points;\n",
    "- Run the model with a larger sample (e.g., 500k tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
